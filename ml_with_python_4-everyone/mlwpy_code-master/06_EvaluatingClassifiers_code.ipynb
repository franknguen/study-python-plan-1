{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from mlwpy import *\n",
    "%matplotlib inline\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "tts = skms.train_test_split(iris.data, iris.target, \n",
    "                            test_size=.33, random_state=21)\n",
    "\n",
    "(iris_train_ftrs, iris_test_ftrs, \n",
    " iris_train_tgt,  iris_test_tgt) = tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal usage:  build-fit-predict-evaluate\n",
    "baseline = dummy.DummyClassifier(strategy=\"most_frequent\")\n",
    "baseline.fit(iris_train_ftrs, iris_train_tgt)\n",
    "base_preds = baseline.predict(iris_test_ftrs)\n",
    "base_acc = metrics.accuracy_score(base_preds, iris_test_tgt)\n",
    "print(base_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['constant', 'uniform', 'stratified', \n",
    "              'prior', 'most_frequent']\n",
    "\n",
    "# setup args to create diff. DummyClassifier strategies\n",
    "baseline_args = [{'strategy':s} for s in strategies]\n",
    "baseline_args[0]['constant'] = 0 # class 0 is setosa \n",
    "\n",
    "accuracies = []\n",
    "for bla in baseline_args:\n",
    "    baseline = dummy.DummyClassifier(**bla)\n",
    "    baseline.fit(iris_train_ftrs, iris_train_tgt)\n",
    "    base_preds = baseline.predict(iris_test_ftrs)\n",
    "    accuracies.append(metrics.accuracy_score(base_preds, iris_test_tgt))\n",
    "    \n",
    "display(pd.DataFrame({'accuracy':accuracies}, index=strategies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful stdlib tool for cleaning up printouts\n",
    "import textwrap\n",
    "print(textwrap.fill(str(sorted(metrics.SCORERS.keys())), \n",
    "                    width=70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier()\n",
    "\n",
    "# help(knn.score) # verbose, but complete\n",
    "\n",
    "print(knn.score.__doc__.splitlines()[0])\n",
    "print('\\n---and---\\n')\n",
    "print(\"\\n\".join(knn.score.__doc__.splitlines()[-6:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_preds = (neighbors.KNeighborsClassifier()\n",
    "                      .fit(iris_train_ftrs, iris_train_tgt)\n",
    "                      .predict(iris_test_ftrs))\n",
    "\n",
    "print(\"accuracy:\", metrics.accuracy_score(iris_test_tgt, \n",
    "                                          tgt_preds))\n",
    "\n",
    "cm = metrics.confusion_matrix(iris_test_tgt, \n",
    "                              tgt_preds)\n",
    "print(\"confusion matrix:\", cm, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "cm = metrics.confusion_matrix(iris_test_tgt, tgt_preds)\n",
    "ax = sns.heatmap(cm, annot=True, square=True,\n",
    "                 xticklabels=iris.target_names, \n",
    "                 yticklabels=iris.target_names)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_prec = metrics.precision_score(iris_test_tgt, \n",
    "                                     tgt_preds, \n",
    "                                     average='macro')\n",
    "print(\"macro:\", macro_prec)\n",
    "\n",
    "cm = metrics.confusion_matrix(iris_test_tgt, tgt_preds)\n",
    "n_labels = len(iris.target_names)\n",
    "print(\"should equal 'macro avg':\", \n",
    "      # correct           column              # columns\n",
    "      (np.diag(cm) / cm.sum(axis=0)).sum() / n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"micro:\", metrics.precision_score(iris_test_tgt, \n",
    "                                        tgt_preds, \n",
    "                                        average='micro'))\n",
    "\n",
    "cm = metrics.confusion_matrix(iris_test_tgt, tgt_preds)\n",
    "print(\"should equal avg='micro':\", \n",
    "      # TP.sum()        / (TP&FP).sum() --> \n",
    "      # all correct     / all preds\n",
    "      np.diag(cm).sum() / cm.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(iris_test_tgt, \n",
    "                                    tgt_preds))\n",
    "# average is a weighted macro average (see text)\n",
    "\n",
    "# verify sums-across-rows\n",
    "cm = metrics.confusion_matrix(iris_test_tgt, tgt_preds)\n",
    "print(\"row counts equal support:\", cm.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: this is 1 \"one\" not l \"ell\"\n",
    "is_versicolor = iris.target == 1\n",
    "tts_1c = skms.train_test_split(iris.data, is_versicolor, \n",
    "                               test_size=.33, random_state = 21)\n",
    "(iris_1c_train_ftrs, iris_1c_test_ftrs, \n",
    " iris_1c_train_tgt,  iris_1c_test_tgt) = tts_1c\n",
    "\n",
    "# build, fit, predict (probability scores) for NB model\n",
    "gnb = naive_bayes.GaussianNB()\n",
    "prob_true = (gnb.fit(iris_1c_train_ftrs, iris_1c_train_tgt)\n",
    "                .predict_proba(iris_1c_test_ftrs)[:,1]) # [:,1]==\"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresh = metrics.roc_curve(iris_1c_test_tgt, \n",
    "                                     prob_true)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"FPR : {}\".format(fpr), \n",
    "      \"TPR : {}\".format(tpr), sep='\\n')\n",
    "\n",
    "\n",
    "# create the main graph\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.plot(fpr, tpr, 'o--')\n",
    "ax.set_title(\"1-Class Iris ROC Curve\\nAUC:{:.3f}\".format(auc))\n",
    "ax.set_xlabel(\"FPR\") \n",
    "ax.set_ylabel(\"TPR\");\n",
    "\n",
    "# do a bit of work to label some points with their\n",
    "# respective thresholds\n",
    "investigate = np.array([1,3,5])\n",
    "for idx in investigate:\n",
    "    th, f, t = thresh[idx], fpr[idx], tpr[idx]\n",
    "    ax.annotate('thresh = {:.3f}'.format(th), \n",
    "                xy=(f+.01, t-.01), xytext=(f+.1, t),\n",
    "                arrowprops = {'arrowstyle':'->'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_fmt = \"Threshold {}\\n~{:5.3f}\\nTPR : {:.3f}\\nFPR : {:.3f}\"\n",
    "\n",
    "pn = ['Positive', 'Negative']\n",
    "add_args = {'xticklabels': pn,\n",
    "            'yticklabels': pn,\n",
    "            'square':True}\n",
    "\n",
    "fig, axes = plt.subplots(1,3, sharey = True, figsize=(12,4))\n",
    "for ax, thresh_idx in zip(axes.flat, investigate):\n",
    "    preds_at_th = prob_true < thresh[thresh_idx]\n",
    "    cm = metrics.confusion_matrix(1-iris_1c_test_tgt, preds_at_th)\n",
    "    sns.heatmap(cm, annot=True, cbar=False, ax=ax,\n",
    "                **add_args)\n",
    "\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_title(title_fmt.format(thresh_idx, \n",
    "                                  thresh[thresh_idx],\n",
    "                                  tpr[thresh_idx], \n",
    "                                  fpr[thresh_idx]))\n",
    "\n",
    "axes[0].set_ylabel('Actual');\n",
    "# note: e.g. for threshold 3\n",
    "# FPR = 1-spec = 1 - 31/(31+2) = 1 - 31/33 = 0.0606..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
    "model = neighbors.KNeighborsClassifier(3)\n",
    "cv_auc = skms.cross_val_score(model, iris.data, iris.target==1, \n",
    "                              scoring='roc_auc', cv=10)\n",
    "ax = sns.swarmplot(cv_auc, orient='v')\n",
    "ax.set_title('10-Fold AUCs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkout = [0,50,100]\n",
    "print(\"Original Encoding\")\n",
    "print(iris.target[checkout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'Multi-label' Encoding\")\n",
    "print(skpre.label_binarize(iris.target, [0,1,2])[checkout])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_multi_tgt = skpre.label_binarize(iris.target, [0,1,2])\n",
    "\n",
    "# im --> \"iris multi\"\n",
    "(im_train_ftrs, im_test_ftrs, \n",
    " im_train_tgt,  im_test_tgt) = skms.train_test_split(iris.data, \n",
    "                                                     iris_multi_tgt,\n",
    "                                                     test_size=.33,\n",
    "                                                     random_state=21)\n",
    "\n",
    "# knn wrapped up in one-versus-rest (3 classifiers)\n",
    "knn        = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "ovr_knn    = skmulti.OneVsRestClassifier(knn) \n",
    "pred_probs = (ovr_knn.fit(im_train_ftrs, im_train_tgt)\n",
    "                     .predict_proba(im_test_ftrs))\n",
    "\n",
    "# make ROC plots\n",
    "lbl_fmt = \"Class {} vs Rest (AUC = {:.2f})\"\n",
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "for cls in [0,1,2]:\n",
    "    fpr, tpr, _ = metrics.roc_curve(im_test_tgt[:,cls], \n",
    "                                    pred_probs[:,cls])\n",
    "    label = lbl_fmt.format(cls, metrics.auc(fpr,tpr))\n",
    "    ax.plot(fpr, tpr, 'o--', label=label)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn         = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "ovo_knn     = skmulti.OneVsOneClassifier(knn) \n",
    "pred_scores = (ovo_knn.fit(iris_train_ftrs, iris_train_tgt)\n",
    "                     .decision_function(iris_test_ftrs))\n",
    "df = pd.DataFrame(pred_scores)\n",
    "df['class'] = df.values.argmax(axis=1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: ugly to make column headers\n",
    "mi = pd.MultiIndex([['Class Indicator', 'Vote'], [0, 1, 2]],\n",
    "                    [[0]*3+[1]*3,list(range(3)) * 2])\n",
    "df = pd.DataFrame(np.c_[im_test_tgt, pred_scores], \n",
    "                  columns=mi)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_and_till_M_statistic(test_tgt, test_probs, weighted=False):\n",
    "    def auc_helper(truth, probs):\n",
    "        fpr, tpr, _ = metrics.roc_curve(truth, probs)\n",
    "        return metrics.auc(fpr, tpr)\n",
    "\n",
    "    classes   = np.unique(test_tgt)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    indicator = skpre.label_binarize(test_tgt, classes)\n",
    "    avg_auc_sum = 0.0\n",
    "\n",
    "    # comparing class i and class j\n",
    "    for ij in it.combinations(classes, 2):\n",
    "        # use use sum to act like a logical or\n",
    "        ij_indicator = indicator[:,ij].sum(axis=1, \n",
    "                                           dtype=np.bool)\n",
    "        \n",
    "        # slightly ugly, can't broadcast these as indexes\n",
    "        # use .ix_ to save the day\n",
    "        ij_probs    = test_probs[np.ix_(ij_indicator, ij)]\n",
    "        ij_test_tgt = test_tgt[ij_indicator]\n",
    "\n",
    "        i,j = ij\n",
    "        auc_ij = auc_helper(ij_test_tgt==i, ij_probs[:,0]) \n",
    "        auc_ji = auc_helper(ij_test_tgt==j, ij_probs[:,1]) \n",
    "\n",
    "        # compared to Hand & Till reference\n",
    "        # no / 2 ... factor it out since it will cancel\n",
    "        avg_auc_ij = (auc_ij + auc_ji) \n",
    "\n",
    "        if weighted:\n",
    "            avg_auc_ij *= ij_indicator.sum() / len(test_tgt)\n",
    "        avg_auc_sum += avg_auc_ij\n",
    "\n",
    "    # compared to Hand & Till reference\n",
    "    # no * 2 ... factored out above and they cancel\n",
    "    M = avg_auc_sum / (n_classes * (n_classes-1)) \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(iris_train_ftrs, iris_train_tgt)\n",
    "test_probs = knn.predict_proba(iris_test_ftrs)\n",
    "hand_and_till_M_statistic(iris_test_tgt, test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
    "htm_scorer = metrics.make_scorer(hand_and_till_M_statistic, \n",
    "                                 needs_proba=True)\n",
    "cv_auc = skms.cross_val_score(model, \n",
    "                              iris.data, iris.target, \n",
    "                              scoring=htm_scorer, cv=10)\n",
    "sns.swarmplot(cv_auc, orient='v')\n",
    "ax.set_title('10-Fold H&T Ms');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(6,3))\n",
    "for cls in [0,1,2]:\n",
    "    prc = metrics.precision_recall_curve\n",
    "    precision, recall, _ = prc(im_test_tgt[:,cls], \n",
    "                               pred_probs[:,cls])\n",
    "    prc_auc = metrics.auc(recall, precision)\n",
    "    label = \"Class {} vs Rest (AUC) = {:.2f})\".format(cls, prc_auc) \n",
    "    ax.plot(recall, precision, 'o--', label=label)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negate b/c we want big values first\n",
    "myorder = np.argsort(-prob_true)\n",
    "\n",
    "# cumulative sum then to percent (last value is total)\n",
    "realpct_myorder = iris_1c_test_tgt[myorder].cumsum()       \n",
    "realpct_myorder = realpct_myorder / realpct_myorder[-1]\n",
    "\n",
    "# convert counts of data into percents\n",
    "N = iris_1c_test_tgt.size\n",
    "xs = np.linspace(1/N,1,N)\n",
    "\n",
    "print(myorder[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(8,4))\n",
    "fig.tight_layout()\n",
    "\n",
    "# cumulative response\n",
    "ax1.plot(xs, realpct_myorder, 'r.')\n",
    "ax1.plot(xs, xs, 'b-')\n",
    "ax1.axes.set_aspect('equal')\n",
    "\n",
    "ax1.set_title(\"Cumulative Response\")\n",
    "ax1.set_ylabel(\"Percent of Actual Hits\")\n",
    "ax1.set_xlabel(\"Percent Of Population\\n\" +\n",
    "               \"Starting with Highest Predicted Hits\")\n",
    "\n",
    "# lift\n",
    "# replace divide by zero with 1.0\n",
    "ax2.plot(xs, realpct_myorder / np.where(xs > 0, xs, 1))\n",
    "\n",
    "ax2.set_title(\"Lift Versus Random\")\n",
    "ax2.set_ylabel(\"X-Fold Improvement\") # not cross-fold!\n",
    "ax2.set_xlabel(\"Percent Of Population\\n\" + \n",
    "               \"Starting with Highest Predicted Hits\")\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.yaxis.set_label_position('right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'base'  : baseline,\n",
    "               'gnb'   : naive_bayes.GaussianNB(),\n",
    "               '3-NN'  : neighbors.KNeighborsClassifier(n_neighbors=3),\n",
    "               '10-NN' : neighbors.KNeighborsClassifier(n_neighbors=10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the one_class iris problem so we don't have random ==1 around\n",
    "iris_onec_ftrs = iris.data\n",
    "iris_onec_tgt  = iris.target==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msrs = ['accuracy', 'average_precision', 'roc_auc']\n",
    "\n",
    "fig, axes = plt.subplots(len(msrs), 1, figsize=(6, 2*len(msrs)))\n",
    "fig.tight_layout()\n",
    "\n",
    "for mod_name, model in classifiers.items():\n",
    "    # abbreviate\n",
    "    cvs = skms.cross_val_score\n",
    "    cv_results = {msr:cvs(model, iris_onec_ftrs, iris_onec_tgt,\n",
    "                          scoring=msr, cv=10) for msr in msrs}\n",
    "    \n",
    "    for ax, msr in zip(axes, msrs):\n",
    "        msr_results = cv_results[msr]\n",
    "        my_lbl = \"{:12s} {:.3f} {:.2f}\".format(mod_name, \n",
    "                                               msr_results.mean(), \n",
    "                                               msr_results.std())\n",
    "        ax.plot(msr_results, 'o--', label=my_lbl)\n",
    "        ax.set_title(msr)\n",
    "        ax.legend(loc='lower center', ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(4,4), sharex=True, sharey=True)\n",
    "fig.tight_layout()\n",
    "\n",
    "for ax, (mod_name, model) in zip(axes.flat, classifiers.items()):\n",
    "    preds = skms.cross_val_predict(model, \n",
    "                                   iris_onec_ftrs, iris_onec_tgt, \n",
    "                                   cv=10)\n",
    "    \n",
    "    cm = metrics.confusion_matrix(iris.target==1, preds)\n",
    "    sns.heatmap(cm, annot=True, ax=ax, \n",
    "                cbar=False, square=True, fmt=\"d\")\n",
    "    \n",
    "    ax.set_title(mod_name)\n",
    "    \n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,1].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "axes[1,0].set_ylabel('Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,4))\n",
    "\n",
    "cv_prob_true = {}\n",
    "for mod_name, model in classifiers.items():\n",
    "    cv_probs = skms.cross_val_predict(model, \n",
    "                                      iris_onec_ftrs, iris_onec_tgt, \n",
    "                                      cv=10, method='predict_proba')\n",
    "    cv_prob_true[mod_name] = cv_probs[:,1]\n",
    "    \n",
    "    fpr, tpr, thresh = metrics.roc_curve(iris_onec_tgt, \n",
    "                                         cv_prob_true[mod_name])\n",
    "    \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, 'o--', label=\"{}:{}\".format(mod_name, auc))\n",
    "\n",
    "ax.set_title('ROC Curves')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "N = len(iris_onec_tgt)\n",
    "xs = np.linspace(1/N,1,N)\n",
    "\n",
    "ax1.plot(xs, xs, 'b-')\n",
    "\n",
    "for mod_name in classifiers:    \n",
    "    # negate b/c we want big values first\n",
    "    myorder = np.argsort(-cv_prob_true[mod_name])\n",
    "\n",
    "    # cumulative sum then to percent (last value is total)\n",
    "    realpct_myorder = iris_onec_tgt[myorder].cumsum()       \n",
    "    realpct_myorder = realpct_myorder / realpct_myorder[-1]\n",
    "    \n",
    "    ax1.plot(xs, realpct_myorder, '.', label=mod_name)\n",
    "    \n",
    "    ax2.plot(xs, \n",
    "            realpct_myorder / np.where(xs > 0, xs, 1),\n",
    "            label=mod_name)\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "ax1.set_title(\"Cumulative Response\")\n",
    "ax2.set_title(\"Lift versus Random\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = pd.read_csv('data/portugese_student_numeric_discrete.csv')\n",
    "student_df['grade'] = pd.Categorical(student_df['grade'], \n",
    "                                     categories=['low', 'mid', 'high'], \n",
    "                                     ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_ftrs = student_df[student_df.columns[:-1]]\n",
    "student_tgt  = student_df['grade'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
    "model = neighbors.KNeighborsClassifier(3)\n",
    "cv_auc = skms.cross_val_score(model, \n",
    "                              student_ftrs, student_tgt, \n",
    "                              scoring='accuracy', cv=10)\n",
    "ax = sns.swarmplot(cv_auc, orient='v')\n",
    "ax.set_title('10-Fold Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsClassifier(3)\n",
    "my_scorer = metrics.make_scorer(metrics.precision_score,\n",
    "                                average='macro')\n",
    "cv_auc = skms.cross_val_score(model, \n",
    "                              student_ftrs, student_tgt, \n",
    "                              scoring=my_scorer, cv=10)\n",
    "fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
    "sns.swarmplot(cv_auc, orient='v')\n",
    "ax.set_title('10-Fold Macro Precision');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm_scorer = metrics.make_scorer(hand_and_till_M_statistic, \n",
    "                                 needs_proba=True)\n",
    "cv_auc = skms.cross_val_score(model, \n",
    "                              student_ftrs, student_tgt, \n",
    "                              scoring=htm_scorer, cv=10)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(3,3))\n",
    "sns.swarmplot(cv_auc, orient='v')\n",
    "ax.set_title('10-Fold H&T Ms');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'base'  : dummy.DummyClassifier(strategy=\"most_frequent\"),\n",
    "               'gnb'   : naive_bayes.GaussianNB(),\n",
    "               '3-NN'  : neighbors.KNeighborsClassifier(n_neighbors=10),\n",
    "               '10-NN' : neighbors.KNeighborsClassifier(n_neighbors=3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_precision = metrics.make_scorer(metrics.precision_score,\n",
    "                                      average='macro')\n",
    "macro_recall    = metrics.make_scorer(metrics.recall_score,\n",
    "                                      average='macro')\n",
    "htm_scorer = metrics.make_scorer(hand_and_till_M_statistic, \n",
    "                                 needs_proba=True)\n",
    "\n",
    "msrs = ['accuracy', macro_precision, \n",
    "        macro_recall, htm_scorer]\n",
    "\n",
    "fig, axes = plt.subplots(len(msrs), 1, figsize=(6, 2*len(msrs)))\n",
    "fig.tight_layout()\n",
    "\n",
    "for mod_name, model in classifiers.items():\n",
    "    # abbreviate\n",
    "    cvs = skms.cross_val_score\n",
    "    cv_results = {msr:cvs(model, student_ftrs, student_tgt,\n",
    "                          scoring=msr, cv=10) for msr in msrs}\n",
    "    \n",
    "    for ax, msr in zip(axes, msrs):\n",
    "        msr_results = cv_results[msr]\n",
    "        my_lbl = \"{:12s} {:.3f} {:.2f}\".format(mod_name, \n",
    "                                               msr_results.mean(), \n",
    "                                               msr_results.std())\n",
    "        ax.plot(msr_results, 'o--')\n",
    "        ax.set_title(msr)\n",
    "        # uncomment to see summary stats (clutters plots)\n",
    "        #ax.legend(loc='lower center') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(5,5), sharex=True, sharey=True)\n",
    "fig.tight_layout()\n",
    "\n",
    "for ax, (mod_name, model) in zip(axes.flat, \n",
    "                                 classifiers.items()):\n",
    "    preds = skms.cross_val_predict(model, \n",
    "                                   student_ftrs, student_tgt, \n",
    "                                   cv=10)\n",
    "    \n",
    "    cm = metrics.confusion_matrix(student_tgt, preds)\n",
    "    sns.heatmap(cm, annot=True, ax=ax, \n",
    "                cbar=False, square=True, fmt=\"d\",\n",
    "                xticklabels=['low', 'med', 'high'],\n",
    "                yticklabels=['low', 'med', 'high'])\n",
    "    \n",
    "    ax.set_title(mod_name)\n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,1].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "axes[1,0].set_ylabel('Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_url = ('https://archive.ics.uci.edu/' + \n",
    "               'ml/machine-learning-databases/00320/student.zip')\n",
    "def grab_student_numeric_discrete():\n",
    "    # download zip file and unzip\n",
    "    # unzipping unknown files can be a security hazard\n",
    "    import urllib.request, zipfile\n",
    "    urllib.request.urlretrieve(student_url,\n",
    "                               'port_student.zip')\n",
    "    zipfile.ZipFile('port_student.zip').extract('student-mat.csv')\n",
    "\n",
    "    # preprocessing\n",
    "    df = pd.read_csv('student-mat.csv', sep=';')\n",
    "    \n",
    "    # g1 & g2 are highly correlated with g3;\n",
    "    # dropping them makes the problem sig. harder\n",
    "    # we also remove all non-numeric columns\n",
    "    # and discretize the final grade by 0-50-75-100 percentile\n",
    "    # which were determined by hand\n",
    "    df = df.drop(columns=['G1', 'G2']).select_dtypes(include=['number'])\n",
    "    df['grade'] = pd.cut(df['G3'], [0, 11, 14, 20], \n",
    "                         labels=['low', 'mid', 'high'],\n",
    "                         include_lowest=True)\n",
    "    df.drop(columns=['G3'], inplace=True)\n",
    "\n",
    "    # save as\n",
    "    df.to_csv('portugese_student_numeric_discrete.csv', index=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
