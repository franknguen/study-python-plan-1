{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from mlwpy import *\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "digits_ftrs, digits_tgt = digits.data, digits.target\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_ftrs, diabetes_tgt = diabetes.data, diabetes.target\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "tts = skms.train_test_split(iris.data, iris.target, \n",
    "                            test_size=.75, stratify=iris.target)\n",
    "(iris_train_ftrs, iris_test_ftrs, \n",
    " iris_train_tgt,  iris_test_tgt) = tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = [linear_model.LogisticRegression(),\n",
    "                   tree.DecisionTreeClassifier(max_depth=3),\n",
    "                   naive_bayes.GaussianNB()]\n",
    "base_estimators = [(get_model_name(m), m) for m in base_estimators]\n",
    "\n",
    "ensemble_model = ensemble.VotingClassifier(estimators=base_estimators)\n",
    "skms.cross_val_score(ensemble_model, digits_ftrs, digits_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([1,5,10,10,17,20,35])\n",
    "def compute_mean(data):\n",
    "    return np.sum(data) / data.size\n",
    "compute_mean(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(data):\n",
    "    N   = len(data)\n",
    "    idx = np.arange(N)\n",
    "    bs_idx = np.random.choice(idx, N, \n",
    "                              replace=True) # default added for clarity\n",
    "    return data[bs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsms = []\n",
    "for i in range(5):\n",
    "    bs_sample = bootstrap_sample(dataset)\n",
    "    bs_mean = compute_mean(bs_sample)\n",
    "    bsms.append(bs_mean)\n",
    "    \n",
    "    print(bs_sample, \"{:5.2f}\".format(bs_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:5.2f}\".format(sum(bsms) / len(bsms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bootstrap_statistic(data, num_boots, statistic):\n",
    "    ' repeatedly calculate statistic on num_boots bootstrap samples'\n",
    "    # no comments from the peanut gallery\n",
    "    bs_stats = [statistic(bootstrap_sample(data)) for i in range(num_boots)]\n",
    "    # return the average of the calculated statistics\n",
    "    return np.sum(bs_stats) / num_boots\n",
    "\n",
    "bs_mean = compute_bootstrap_statistic(dataset, 100, compute_mean)\n",
    "print(\"{:5.2f}\".format(bs_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_knn_statistic(new_example):\n",
    "    def knn_statistic(dataset):\n",
    "        ftrs, tgt = dataset[:,:-1], dataset[:,-1]\n",
    "        knn = neighbors.KNeighborsRegressor(n_neighbors=3).fit(ftrs, tgt)\n",
    "        return knn.predict(new_example)\n",
    "    return knn_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to slightly massage data for this scenario\n",
    "# we use last example as our fixed test example\n",
    "diabetes_dataset = np.c_[diabetes_ftrs, diabetes_tgt]\n",
    "ks = make_knn_statistic(diabetes_ftrs[-1].reshape(1,-1))\n",
    "compute_bootstrap_statistic(diabetes_dataset, 100, ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagged_learner(dataset, base_model, num_models=10):\n",
    "    # pseudo-code:  needs tweaks to run\n",
    "    models = []\n",
    "    for n in num_models:\n",
    "        bs_sample = np.random.choice(dataset, N, replace=True)\n",
    "        models.append(base_model().fit(*bs_sample))\n",
    "    return models\n",
    "\n",
    "def bagged_predict_class(models, example):\n",
    "    # take the most frequent (mode) predicted class as result\n",
    "    preds = [m.predict(example) for m in models]\n",
    "    return pd.Series(preds).mode() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_boosted_classifier(base_classifier, bc_args, \n",
    "                          examples, targets, M):\n",
    "    N = len(examples)\n",
    "    data_weights = np.full(N, 1/N)\n",
    "    models, model_weights = [], []\n",
    "\n",
    "    for i in range(M):\n",
    "        weighted_dataset = reweight((examples,targets), \n",
    "                                    data_weights)\n",
    "        this_model = base_classifier(*bc_args).fit(*weighted_dataset)\n",
    "\n",
    "        errors = this_model.predict(examples) != targets\n",
    "        weighted_error = np.dot(weights, errors)\n",
    "        \n",
    "        # magic reweighting steps\n",
    "        this_model_wgt = np.log(1-weighted_error)/weighted_error\n",
    "        data_weights   *= np.exp(this_model_wgt * errors)\n",
    "        data_weights   /= data_weights.sum() # normalize to 1.0\n",
    "        \n",
    "        models.append(this_model)\n",
    "        model_weights.append(this_model_wgt)\n",
    "        \n",
    "    return ensemble.VotingClassifier(models, \n",
    "                                     voting='soft', \n",
    "                                     weights=model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ensemble.AdaBoostClassifier()\n",
    "stage_preds = (model.fit(iris_train_ftrs, iris_train_tgt)\n",
    "                    .staged_predict(iris_test_ftrs))\n",
    "stage_scores = [metrics.accuracy_score(iris_test_tgt,\n",
    "                                       pred) for pred in stage_preds]\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(stage_scores)\n",
    "ax.set_xlabel('# steps')\n",
    "ax.set_ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_score(model, ds):\n",
    "    return skms.cross_val_score(model, *ds, cv=10).mean()\n",
    "\n",
    "stump  = tree.DecisionTreeClassifier(max_depth=1)\n",
    "dtree  = tree.DecisionTreeClassifier(max_depth=3)\n",
    "forest = ensemble.RandomForestClassifier(max_features=1, max_depth=1)\n",
    "\n",
    "tree_classifiers = {'stump' : stump, 'dtree' : dtree, 'forest': forest}\n",
    "\n",
    "max_est = 100\n",
    "data = (digits_ftrs, digits_tgt)\n",
    "stump_score   = fit_predict_score(stump, data)\n",
    "tree_score    = fit_predict_score(dtree, data)\n",
    "forest_scores = [fit_predict_score(forest.set_params(n_estimators=n),\n",
    "                                   data) \n",
    "                 for n in range(1,max_est+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "xs = list(range(1,max_est+1))\n",
    "ax.plot(xs, np.repeat(stump_score, max_est), label='stump')\n",
    "ax.plot(xs, np.repeat(tree_score, max_est),  label='tree')\n",
    "ax.plot(xs, forest_scores, label='forest')\n",
    "\n",
    "ax.set_xlabel('Number of Trees in Forest')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_manual_cv(dataset, k=10):\n",
    "    ' manually generate cv-folds from dataset '\n",
    "    # expect ftrs, tgt tuple\n",
    "    ds_ftrs, ds_tgt = dataset\n",
    "    manual_cv = skms.StratifiedKFold(k).split(ds_ftrs, \n",
    "                                              ds_tgt)\n",
    "    for (train_idx, test_idx) in manual_cv:\n",
    "        train_ftrs = ds_ftrs[train_idx]\n",
    "        test_ftrs  = ds_ftrs[test_idx]\n",
    "        train_tgt = ds_tgt[train_idx]\n",
    "        test_tgt  = ds_tgt[test_idx]\n",
    "        \n",
    "        yield (train_ftrs, test_ftrs,\n",
    "               train_tgt, test_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBC  = ensemble.AdaBoostClassifier\n",
    "GradBC = ensemble.GradientBoostingClassifier\n",
    "boosted_classifiers = {'boost(Ada)' : AdaBC(learning_rate=2.0),\n",
    "                       'boost(Grad)' : GradBC(loss=\"deviance\")}\n",
    "mean_accs = {}\n",
    "for name, model in boosted_classifiers.items():\n",
    "    model.set_params(n_estimators=max_est)\n",
    "    accs = []\n",
    "    for tts in my_manual_cv((digits_ftrs, digits_tgt)):\n",
    "        train_f, test_f, train_t, test_t = tts\n",
    "        s_preds = (model.fit(train_f, train_t)\n",
    "                        .staged_predict(test_f))\n",
    "        s_scores = [metrics.accuracy_score(test_t, p) for p in s_preds]\n",
    "        accs.append(s_scores)\n",
    "    mean_accs[name] = np.array(accs).mean(axis=0)\n",
    "mean_acc_df = pd.DataFrame.from_dict(mean_accs,orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = list(range(1,max_est+1))\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8,3),sharey=True)\n",
    "ax1.plot(xs, np.repeat(stump_score, max_est), label='stump')\n",
    "ax1.plot(xs, np.repeat(tree_score, max_est),  label='tree')\n",
    "ax1.plot(xs, forest_scores, label='forest')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xlabel('Number of Trees in Forest')\n",
    "ax1.legend()\n",
    "\n",
    "mean_acc_df.plot(ax=ax2)\n",
    "ax2.set_ylim(0.0, 1.1)\n",
    "ax2.set_xlabel('# Iterations')\n",
    "ax2.legend(ncol=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install py-xgboost\n",
    "import xgboost\n",
    "# gives us xgboost.XGBRegressor, xgboost.XGBClassifier\n",
    "# which interface nicely with sklearn\n",
    "# see docs at\n",
    "# http://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "xgbooster = xgboost.XGBClassifier(objective=\"multi:softmax\")\n",
    "scores = skms.cross_val_score(xgbooster, iris.data, iris.target, cv=10)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
