{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from mlwpy import *\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"the cat in the hat\",\n",
    "        \"the cow jumped over the moon\",\n",
    "        \"the cat mooed and the cow meowed\",\n",
    "        \"the cat said to the cow cow you are not a cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(\" \".join(docs).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = set(['a', 'to', 'the', 'in', 'and', 'are'])\n",
    "vocabulary = vocabulary - common_words\n",
    "print(textwrap.fill(str(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {k:v for k in lst} creates a dictionary from keys:values\n",
    "# it is called a \"dictionary comprehension\"\n",
    "doc_contains = [{w:(w in d) for w in vocabulary} for d in docs]\n",
    "display(pd.DataFrame(doc_contains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = ['cat', 'cow', 'hat', 'jumped', 'meowed']\n",
    "c2 = ['mooed', 'moon', 'not', 'over', 'said', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_contains = [{w:(w in d) for w in vocabulary} for d in docs]\n",
    "display(pd.DataFrame(doc_contains)[c1])\n",
    "display(pd.DataFrame(doc_contains)[c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = [{w:d.count(w) for w in vocabulary} for d in docs]\n",
    "wcs = pd.DataFrame(word_count)\n",
    "display(wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sk_txt\n",
    "sparse = sk_txt.CountVectorizer(stop_words='english').fit_transform(docs)\n",
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wcs.values.sum(axis=0, keepdims=True)\n",
    "doc_freq = pd.DataFrame(wcs.astype(np.bool).sum(axis='rows')).T\n",
    "display(doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = np.log(len(docs) / doc_freq) \n",
    "#  == np.log(len(docs)) - np.log(doc_freq)\n",
    "display(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf  = wcs * idf.iloc[0]   # aligns columns for multiplication\n",
    "display(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpre.Normalizer(norm='l1').fit_transform(wcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = (sk_txt.TfidfVectorizer(norm='l1', stop_words='english')\n",
    "                .fit_transform(docs))\n",
    "sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the groups:\")\n",
    "print(textwrap.fill(str(twenty_train.target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].splitlines()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_vect     = sk_txt.CountVectorizer()\n",
    "tfidf_xform = sk_txt.TfidfTransformer()\n",
    "\n",
    "docs_as_counts = ct_vect.fit_transform(twenty_train.data)\n",
    "docs_as_tfidf  = tfidf_xform.fit_transform(docs_as_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = naive_bayes.MultinomialNB().fit(docs_as_tfidf, \n",
    "                                        twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pipeline = pipeline.make_pipeline(sk_txt.CountVectorizer(),\n",
    "                                      sk_txt.TfidfTransformer(),\n",
    "                                      naive_bayes.MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['misc.forsale',\n",
    "              'comp.graphics', \n",
    "              'sci.med', \n",
    "              'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories, \n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)\n",
    "\n",
    "doc_pipeline = pipeline.make_pipeline(sk_txt.TfidfVectorizer(),\n",
    "                                      naive_bayes.MultinomialNB())\n",
    "\n",
    "\n",
    "model = doc_pipeline.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                  categories=categories, \n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)\n",
    "\n",
    "doc_preds = model.predict(twenty_test.data)\n",
    "cm = metrics.confusion_matrix(twenty_test.target, doc_preds)\n",
    "ax = sns.heatmap(cm, annot=True, \n",
    "                 xticklabels=twenty_test.target_names, \n",
    "                 yticklabels=twenty_test.target_names,\n",
    "                 fmt='3d') # cells are counts\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "twod_iris = (decomposition.PCA(n_components=2, whiten=True)\n",
    "                          .fit_transform(iris.data))\n",
    "clusters = cluster.KMeans(n_clusters=3).fit(twod_iris)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(8,4))\n",
    "axes[0].scatter(*twod_iris.T, c=iris.target)\n",
    "axes[1].scatter(*twod_iris.T, c=clusters.labels_)\n",
    "\n",
    "axes[0].set_title(\"Truth\"), axes[1].set_title(\"Clustered\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the data\n",
    "objcat_path = \"./data/101_ObjectCategories\"\n",
    "cat_paths = glob.glob(osp.join(objcat_path, \"*\"))\n",
    "all_categories = [d.split('/')[-1] for d in cat_paths]\n",
    "\n",
    "print(\"number of categories:\", len(all_categories))\n",
    "print(\"first 10 categories:\\n\",\n",
    "      textwrap.fill(str(all_categories[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "\n",
    "test_path = osp.join(objcat_path, 'accordion', 'image_0001.jpg')\n",
    "test_img = imread(test_path)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(2,2))\n",
    "ax.imshow(test_img)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_local_words(img):\n",
    "    ' heavy lifting of creating local visual words from img '\n",
    "    # sift/surf removed from base opencv -> use alternatives\n",
    "    # extractor = cv2.AKAZE_create()\n",
    "    extractor = cv2.ORB_create()\n",
    "    key_points, descriptors = extractor.detectAndCompute(img, None)\n",
    "    return descriptors\n",
    "\n",
    "def id_to_path(img_id):\n",
    "    ' helper to get file location '\n",
    "    cat, num = img_id\n",
    "    return osp.join(objcat_path, cat, \"image_{:04d}.jpg\".format(num))\n",
    "\n",
    "def add_local_words_for_img(local_ftrs, img_id):\n",
    "    ' update local_ftrs inplace '\n",
    "    cat, _ = img_id\n",
    "    img_path = id_to_path(img_id)\n",
    "    img = imread(img_path)\n",
    "    local_ftrs.setdefault(cat, []).append(img_to_local_words(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a few constants\n",
    "use_cats = ['accordion', 'airplanes', 'anchor']\n",
    "use_imgs = range(1,11)\n",
    "\n",
    "img_ids  = list(it.product(use_cats, use_imgs))\n",
    "num_imgs = len(img_ids)\n",
    "\n",
    "global_vocab_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn each img into table of local visual words \n",
    "# (1 table per image, 1 word per row)\n",
    "local_words = {}\n",
    "for img_id in img_ids:\n",
    "    add_local_words_for_img(local_words, img_id)\n",
    "print(local_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itcfi is basically a way to get each individual item from an\n",
    "# iterator of items; it's a long name, so I abbreviate it\n",
    "itcfi = it.chain.from_iterable \n",
    "\n",
    "img_local_word_cts = [lf.shape[0] for lf in itcfi(local_words.values())]\n",
    "print(\"num of local words for images:\")\n",
    "print(textwrap.fill(str(img_local_word_cts), width=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how wide are the local word tables\n",
    "num_local_words = local_words[use_cats[0]][0].shape[1]\n",
    "\n",
    "# how many local words are there total?\n",
    "all_local_words = list(itcfi(local_words.values()))\n",
    "tot_num_local_words = sum(lw.shape[0] for lw in all_local_words)\n",
    "print('total num local words:', tot_num_local_words)\n",
    "\n",
    "# construct joined local tables to perform clustering\n",
    "# np_array_fromiter is described at the end of the chapter\n",
    "lwa_shape = (tot_num_local_words, num_local_words)\n",
    "local_word_arr = np_array_fromiter(itcfi(all_local_words),\n",
    "                                   lwa_shape)\n",
    "print('local word tbl:', local_word_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster (and translate) the local words to global words\n",
    "translator = cluster.KMeans(n_clusters=global_vocab_size)\n",
    "global_words = translator.fit_predict(local_word_arr)\n",
    "print('translated words shape:', global_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which image do the local words belong to\n",
    "# enumerate_outer is descibed at the end of the chapter\n",
    "which_img = enumerate_outer(all_local_words)\n",
    "print('which img len:', len(which_img))\n",
    "\n",
    "# img by global words -> img by histogram\n",
    "counts = co.Counter(zip(which_img, global_words))\n",
    "imgs_as_bogvw = np.zeros((num_imgs, global_vocab_size))\n",
    "for (img, global_word), count in counts.items():\n",
    "    imgs_as_bogvw[img, global_word] = count\n",
    "print('shape hist table:', imgs_as_bogvw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit of a hack; local_ftrs.values() gives \n",
    "# [[img1, img2], [img3, img4, img5], etc.]\n",
    "# answers: what category am i from?\n",
    "img_tgts = enumerate_outer(local_words.values())\n",
    "print('img tgt values:', img_tgts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build learning model\n",
    "std_svc = pipeline.make_pipeline(skpre.StandardScaler(), svm.SVC())\n",
    "svc = std_svc.fit(imgs_as_bogvw, img_tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_example(img_id, translator):\n",
    "    ' from an id, produce an example with global words '\n",
    "    img_local  = img_to_local_words(imread(id_to_path(img_id)))\n",
    "    img_global = translator.predict(img_local)\n",
    "    img_bogvw  = np.bincount(img_global, \n",
    "                             minlength=translator.n_clusters)     \n",
    "    return img_bogvw.reshape(1,-1).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in use_cats:\n",
    "    test = image_to_example((cat, 12), translator)\n",
    "    print(svc.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOVW_XForm:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _to_local_words(self, img_ids):\n",
    "        # turn each img into table of local visual words (1 word per row)\n",
    "        local_words = {}\n",
    "        for img_id in img_ids:\n",
    "            add_local_words_for_img(local_words, img_id)\n",
    "\n",
    "        itcfi = it.chain.from_iterable\n",
    "        all_local_words = list(itcfi(local_words.values()))\n",
    "        return all_local_words\n",
    "    \n",
    "    def fit(self, img_ids, tgt=None):\n",
    "        all_local_words = self._to_local_words(img_ids)\n",
    "        tot_num_local_words = sum(lw.shape[0] for lw in all_local_words)\n",
    "        local_word_arr = np_array_fromiter(itcfi(all_local_words), \n",
    "                                           (tot_num_local_words, num_local_words))\n",
    "        self.translator = cluster.KMeans(n_clusters=global_vocab_size)\n",
    "        self.translator.fit(local_word_arr)\n",
    "        return self\n",
    "\n",
    "    def transform(self, img_ids, tgt=None):\n",
    "        all_local_words = self._to_local_words(img_ids)\n",
    "        tot_num_local_words = sum(lw.shape[0] for lw in all_local_words)\n",
    "        local_word_arr = np_array_fromiter(itcfi(all_local_words), \n",
    "                                           (tot_num_local_words, num_local_words))\n",
    "        global_words = self.translator.predict(local_word_arr)\n",
    "        \n",
    "        # img by global words -> img by histogram\n",
    "        which_img = enumerate_outer(all_local_words)\n",
    "        counts = co.Counter(zip(which_img, global_words))\n",
    "        imgs_as_bogvw = np.zeros((len(img_ids), global_vocab_size))\n",
    "        for (img, global_word), count in counts.items():\n",
    "            imgs_as_bogvw[img, global_word] = count        \n",
    "        return imgs_as_bogvw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cats = ['watch', 'umbrella', 'sunflower', 'kangaroo']\n",
    "use_imgs = range(1,40)\n",
    "\n",
    "img_ids  = list(it.product(use_cats, use_imgs))\n",
    "num_imgs = len(img_ids)\n",
    "\n",
    "# hack\n",
    "cat_id = {c:i for i,c in enumerate(use_cats)}\n",
    "img_tgts = [cat_id[ii[0]] for ii in img_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_img, test_img, \n",
    " train_tgt, test_tgt) = skms.train_test_split(img_ids, img_tgts)\n",
    "bovw_pipe = pipeline.make_pipeline(BOVW_XForm(), \n",
    "                                   skpre.StandardScaler(),\n",
    "                                   svm.SVC())\n",
    "bovw_pipe.fit(train_img, train_tgt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preds = bovw_pipe.predict(test_img)\n",
    "cm = metrics.confusion_matrix(test_tgt, img_preds)\n",
    "ax = sns.heatmap(cm, annot=True, \n",
    "                 xticklabels=use_cats, \n",
    "                 yticklabels=use_cats,\n",
    "                 fmt='3d')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerate_outer(outer_seq):\n",
    "    '''repeat the outer idx based on len of inner'''\n",
    "    return np.repeat(*zip(*enumerate(map(len, outer_seq))))\n",
    "\n",
    "def np_array_fromiter(itr, shape, dtype=np.float64):\n",
    "    ''' helper since np.fromiter only does 1D'''\n",
    "    arr = np.empty(shape, dtype=dtype)\n",
    "    for idx, itm in enumerate(itr):\n",
    "        arr[idx] = itm\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate_outer([[0,1], [10, 20,30], [100,200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_fromiter(enumerate(range(0,50,10)), (5,2))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
