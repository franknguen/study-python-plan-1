{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "from mlwpy import *\n",
    "%matplotlib inline\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "d_tts = skms.train_test_split(diabetes.data,\n",
    "                              diabetes.target, \n",
    "                              test_size=.25,\n",
    "                              random_state=42)\n",
    "\n",
    "(diabetes_train_ftrs, diabetes_test_ftrs, \n",
    " diabetes_train_tgt,  diabetes_test_tgt) = d_tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([3.5, -2.1, .7])\n",
    "print(np.sum(np.abs(weights)),\n",
    "      np.sum(weights**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = np.arange(10)\n",
    "m, b = 3, 2\n",
    "w = np.array([m,b])\n",
    "\n",
    "x = np.c_[x_1, np.repeat(1.0, 10)] # the plus-one trick\n",
    "\n",
    "errors = np.tile(np.array([0.0, 1.0, 1.0, .5, .5]), 2)\n",
    "\n",
    "print(errors * errors)\n",
    "print(np.dot(errors, errors))\n",
    "\n",
    "y_true = rdot(w,x)\n",
    "y_msr  = y_true + errors\n",
    "\n",
    "D = (x,y_msr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(x_1, y_true, 'r', label='true')\n",
    "ax.plot(x_1, y_msr , 'b', label='noisy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_diff(a,b):\n",
    "    return (a-b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rdot(w,x)\n",
    "np.sum(sq_diff(predictions, y_msr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rdot(w,x)\n",
    "\n",
    "loss = np.sum(sq_diff(predictions, y_msr))\n",
    "\n",
    "complexity_1 = np.sum(np.abs(weights))\n",
    "complexity_2 = np.sum(weights**2) # == np.dot(weights, weights)\n",
    "\n",
    "cost_1 = loss + complexity_1\n",
    "cost_2 = loss + complexity_2\n",
    "\n",
    "print(\"Sum(abs) complexity:\", cost_1)\n",
    "print(\"Sum(sqr) complexity:\", cost_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rdot(w,x)\n",
    "errors = np.sum(sq_diff(predictions, y_msr))\n",
    "complexity_1 = np.sum(np.abs(weights))\n",
    "\n",
    "C = .5\n",
    "cost = errors + C * complexity_1\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [linear_model.Lasso(),            # L1 regularized; C=1.0\n",
    "          linear_model.Ridge()]            # L2 regularized; C=1.0\n",
    "\n",
    "for model in models:\n",
    "    model.fit(diabetes_train_ftrs, diabetes_train_tgt)\n",
    "    train_preds = model.predict(diabetes_train_ftrs)\n",
    "    test_preds  = model.predict(diabetes_test_ftrs)\n",
    "    print(get_model_name(model), \n",
    "          \"\\nTrain MSE:\",metrics.mean_squared_error(diabetes_train_tgt, \n",
    "                                                    train_preds), \n",
    "          \"\\n Test MSE:\", metrics.mean_squared_error(diabetes_test_tgt,  \n",
    "                                                     test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  here, we don't ignore small errors\n",
    "error = np.linspace(-4, 4, 100)\n",
    "loss = np.abs(error)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(error, loss)\n",
    "\n",
    "ax.set_xlabel('Raw Error')\n",
    "ax.set_ylabel('Abs Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_error = .75\n",
    "abs_error = abs(an_error)\n",
    "if abs_error < 1.0:\n",
    "    the_loss = 0.0\n",
    "else:\n",
    "    the_loss = abs_error\n",
    "print(the_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_error = 0.75\n",
    "adj_error = abs(an_error) - 1.0\n",
    "if adj_error < 0.0:\n",
    "    the_loss = 0.0\n",
    "else:\n",
    "    the_loss = adj_error\n",
    "print(the_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.linspace(-4, 4, 100)\n",
    "\n",
    "# here, we ignore errors up to 1.0 ... take bigger value\n",
    "loss = np.maximum(np.abs(error) - 1.0, \n",
    "                  np.zeros_like(error))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(error, loss)\n",
    "\n",
    "ax.set_xlabel(\"Raw Error\")\n",
    "ax.set_ylabel(\"Hinge Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2.5\n",
    "\n",
    "xs = np.linspace(-5,5,100)\n",
    "ys_true = 3 * xs + 2\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(xs, ys_true)\n",
    "ax.fill_between(xs, ys_true-threshold, ys_true+threshold, \n",
    "                 color=(1.0,0,0,.25))\n",
    "\n",
    "ax.set_xlabel('Input Feature')\n",
    "ax.set_ylabel('Output Target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2.5\n",
    "\n",
    "xs = np.linspace(-5,5,100)\n",
    "ys = 3 * xs + 2 + np.random.normal(0, 1.5, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(xs, ys, 'o',  color=(0,0,1.0,.5))\n",
    "ax.fill_between(xs, ys_true - threshold, ys_true + threshold, \n",
    "                 color=(1.0,0,0,.25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for the scenario\n",
    "C, epsilon = 1.0, .25\n",
    "\n",
    "# parameters\n",
    "weights = np.array([1.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction, error, loss\n",
    "predictions = rdot(weights, xs.reshape(-1, 1))\n",
    "errors = ys - predictions\n",
    "\n",
    "loss_sse   = np.sum(errors ** 2)\n",
    "loss_sae   = np.sum(np.abs(errors))\n",
    "loss_hinge = np.sum(np.max(np.abs(errors) - epsilon, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complexity penalty for regularization\n",
    "complexity_saw = np.sum(np.abs(weights))\n",
    "complexity_ssw = np.sum(weights**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost\n",
    "cost_gof_regression   = loss_sse   + 0.0\n",
    "cost_L1pen_regression = loss_sse   + C * complexity_saw\n",
    "cost_L2pen_regression = loss_sse   + C * complexity_ssw\n",
    "cost_sv_regression    = loss_hinge + C * complexity_ssw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svrs = [svm.SVR(gamma='auto'),   # default epsilon=0.1 \n",
    "        svm.NuSVR(gamma='auto')] # default nu=0.5\n",
    "\n",
    "for model in svrs:\n",
    "    preds = (model.fit(diabetes_train_ftrs, diabetes_train_tgt)\n",
    "                  .predict(diabetes_test_ftrs))\n",
    "    print(metrics.mean_squared_error(diabetes_test_tgt, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bill = np.linspace(.5, 10.0, 100)\n",
    "collected = np.round(raw_bill)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(raw_bill, collected, '.')\n",
    "ax.set_xlabel(\"raw cost\")\n",
    "ax.set_ylabel(\"bill\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot([0,3],   [0,0],\n",
    "        [3,8],   [5,5],\n",
    "        [8,12],  [2,2],\n",
    "        [12,15], [9,9])\n",
    "ax.set_xticks([3,8,12]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import (check_X_y, \n",
    "                                      check_array, \n",
    "                                      check_is_fitted)\n",
    "\n",
    "class PiecewiseConstantRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, cut_points=None):\n",
    "        self.cut_points = cut_points\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X,y)\n",
    "        assert X.shape[1] == 1 # one variable only\n",
    "        \n",
    "        if self.cut_points is None:\n",
    "            n = (len(X) // 10) + 1\n",
    "            qtiles = np.linspace(0.0, 1.0, n+2)[1:-1]\n",
    "            self.cut_points =  np.percentile(X, qtiles, axis=1)\n",
    "        else:\n",
    "            # ensure cutpoints in-order and in range of X\n",
    "            assert np.all(self.cut_points[:-1] < self.cut_points[1:])\n",
    "            assert (X.min() < self.cut_points[0] and \n",
    "                    self.cut_points[-1] < X.max())\n",
    "\n",
    "        recoded_X = self._recode(X)\n",
    "        # even though the _inner_ model is fit without an intercept\n",
    "        # our piecewise model *does* have an constant term (but see notes)\n",
    "        self.coeffs_ = (linear_model.LinearRegression(fit_intercept=False)\n",
    "                                    .fit(recoded_X, y).coef_)\n",
    "    def _recode(self, X):\n",
    "        cp = self.cut_points\n",
    "        n_pieces = len(cp) + 1\n",
    "        recoded_X = np.eye(n_pieces)[np.searchsorted(cp, X.flat)]\n",
    "        return recoded_X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'coeffs_')\n",
    "        X = check_array(X) \n",
    "        recoded_X = self._recode(X)\n",
    "        return rdot(self.coeffs_, recoded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = np.random.randint(0,10,(100,1)).astype(np.float64)\n",
    "cp = np.array([3,7])\n",
    "tgt = np.searchsorted(cp, ftr.flat) + 1\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.plot(ftr, tgt, '.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we're giving our self all the help we can by using\n",
    "# the same cut points as our data were generated with\n",
    "model = PiecewiseConstantRegression(cut_points=np.array([3, 7]))\n",
    "model.fit(ftr, tgt)\n",
    "preds = model.predict(ftr)\n",
    "print(\"Predictions equal target?\", np.allclose(preds, tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrees = [tree.DecisionTreeRegressor(max_depth=md) for md in [1, 3, 5, 10]]\n",
    "\n",
    "for model in dtrees:\n",
    "    preds = (model.fit(diabetes_train_ftrs, diabetes_train_tgt)\n",
    "                  .predict(diabetes_test_ftrs))\n",
    "    mse = metrics.mean_squared_error(diabetes_test_tgt, preds)\n",
    "    fmt = \"{} {:2d} {:4.0f}\"\n",
    "    print(fmt.format(get_model_name(model),\n",
    "                     model.get_params()['max_depth'],\n",
    "                     mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = pd.read_csv('data/portugese_student_numeric.csv')\n",
    "student_ftrs = student_df[student_df.columns[:-1]]\n",
    "student_tgt  = student_df['G3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_tts = skms.train_test_split(student_ftrs, student_tgt)\n",
    "\n",
    "(student_train_ftrs, student_test_ftrs,\n",
    " student_train_tgt,  student_test_tgt) = student_tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_school = [linear_model.LinearRegression(),\n",
    "              neighbors.KNeighborsRegressor(n_neighbors=3),\n",
    "              neighbors.KNeighborsRegressor(n_neighbors=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1, L2 penalized (abs, sqr) C=1.0 for both\n",
    "penalized_lr = [linear_model.Lasso(), \n",
    "                linear_model.Ridge()]\n",
    "\n",
    "# defaults are epsilon=.1 and nu=.5, respectively\n",
    "svrs = [svm.SVR(), svm.NuSVR()] \n",
    "\n",
    "dtrees = [tree.DecisionTreeRegressor(max_depth=md) for md in [1, 3, 5, 10]]\n",
    "\n",
    "reg_models = old_school + penalized_lr + svrs + dtrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_error(actual, predicted):\n",
    "    ' root-mean-squared-error function '\n",
    "    # lesser values are better (a<b ... a better)\n",
    "    mse = metrics.mean_squared_error(actual, predicted)    \n",
    "    return np.sqrt(mse)\n",
    "rms_scorer = metrics.make_scorer(rms_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = skpre.StandardScaler()\n",
    "\n",
    "scores = {}\n",
    "for model in reg_models:\n",
    "    pipe = pipeline.make_pipeline(scaler, model)\n",
    "    preds = skms.cross_val_predict(pipe, \n",
    "                                   student_ftrs, student_tgt, \n",
    "                                   cv=10)\n",
    "    key = (get_model_name(model) + \n",
    "           str(model.get_params().get('max_depth', \"\")) + \n",
    "           str(model.get_params().get('n_neighbors', \"\")))\n",
    "    scores[key] = rms_error(student_tgt, preds)\n",
    "\n",
    "df = pd.DataFrame.from_dict(scores, orient='index').sort_values(0)\n",
    "df.columns=['RMSE']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_models = [tree.DecisionTreeRegressor(max_depth=1),\n",
    "                 linear_model.Ridge(),\n",
    "                 linear_model.LinearRegression(),\n",
    "                 svm.NuSVR()]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4))\n",
    "for model in better_models:\n",
    "    pipe = pipeline.make_pipeline(scaler, model)    \n",
    "    cv_results = skms.cross_val_score(pipe, \n",
    "                                      student_ftrs, student_tgt, \n",
    "                                      scoring = rms_scorer, \n",
    "                                      cv=10)\n",
    "\n",
    "    my_lbl = \"{:s} ({:5.3f}$\\pm${:.2f})\".format(get_model_name(model), \n",
    "                                                cv_results.mean(), \n",
    "                                                cv_results.std())\n",
    "    ax.plot(cv_results, 'o--', label=my_lbl)\n",
    "    ax.set_xlabel('CV-Fold #')\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.legend()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
